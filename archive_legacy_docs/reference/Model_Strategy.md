# Model Strategy: Development vs Final Iteration (2025)

## Executive Summary

This document provides a **two-phase model strategy** for your intelligent memory system, optimized for the take-home project workflow:

1. **Development Phase**: Cheap models for rapid iteration, testing, and debugging
2. **Final Iteration**: Premium models for the final demo to impress the interviewer

**Only Anthropic (Claude) and OpenAI (GPT) models** are used for simplicity and quality.

---

## Quick Recommendations

### üî® Development Phase (Iteration & Testing)

**Goal**: Minimize costs while building and testing the system.

```yaml
Primary LLM: GPT-4o mini             # $0.15/$0.60 per M
Entity Extraction: GPT-5-nano        # $0.05/$0.40 per M
Embeddings: text-embedding-3-small   # $0.02 per M

Cost per query: ~$0.0042
Total dev cost (150 queries): ~$0.63
```

### üéØ Final Iteration (Demo & Presentation)

**Goal**: Maximum quality to impress the interviewer.

```yaml
Primary LLM: Claude Sonnet 4.5       # $3/$15 per M (with caching)
Entity Extraction: Claude 3.5 Haiku  # $0.80/$4 per M
Embeddings: text-embedding-3-small   # $0.02 per M

Cost per query: ~$0.033 (with caching)
Total demo cost (50 queries): ~$1.65
```

**Total Project Cost**: Development ($0.63) + Final ($1.65) = **$2.28**

---

## Table of Contents

1. [Model Options (Anthropic & OpenAI Only)](#model-options)
2. [Development Phase Strategy](#development-phase)
3. [Final Iteration Strategy](#final-iteration)
4. [Cost Comparison & Projections](#cost-comparison)
5. [Quality Analysis](#quality-analysis)
6. [Implementation Guide](#implementation-guide)
7. [Migration Path](#migration-path)

---

## Model Options (Anthropic & OpenAI Only)

### OpenAI GPT-5 Family (Released August 2025)

| Model | Input $/M | Output $/M | Cached $/M | Context | Quality | Use Case |
|-------|-----------|------------|------------|---------|---------|----------|
| **GPT-5** | $1.25 | $10.00 | $0.125 | 256K-1M | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | Production primary |
| **GPT-5-mini** | $0.25 | $2.00 | $0.025 | 256K | ‚≠ê‚≠ê‚≠ê‚≠ê | Dev primary, prod fast ops |
| **GPT-5-nano** | $0.05 | $0.40 | $0.005 | 256K | ‚≠ê‚≠ê‚≠ê | Dev fast ops, simple tasks |
| **GPT-4o** | $2.50 | $10.00 | N/A | 128K | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | Legacy option |
| **GPT-4o mini** | $0.15 | $0.60 | N/A | 128K | ‚≠ê‚≠ê‚≠ê‚≠ê | **Dev primary (best value)** |

**Key Features**:
- **GPT-5 Reasoning Router**: Auto-switches between fast/thinking modes
- **90% cache discount**: Repeated content costs 10√ó less
- **Unified ecosystem**: Single API for all models

### Anthropic Claude 4 Family (2025)

| Model | Input $/M | Output $/M | Cached $/M | Context | Quality | Use Case |
|-------|-----------|------------|------------|---------|---------|----------|
| **Claude Opus 4.1** | $15.00 | $75.00 | $1.50 | 200K | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | Ultra-premium only |
| **Claude Sonnet 4.5** | $3.00 | $15.00 | $0.30 | 1M | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | **Final iteration primary** |
| **Claude Sonnet 4** | $3.00 | $15.00 | $0.30 | 1M | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | Alternative to 4.5 |
| **Claude 3.5 Haiku** | $0.80 | $4.00 | $0.08 | 200K | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | **Final iteration fast ops** |
| **Claude 3.5 Sonnet** | $3.00 | $15.00 | $0.30 | 200K | ‚≠ê‚≠ê‚≠ê‚≠ê | Legacy option |

**Key Features**:
- **77.2% SWE-bench** (Sonnet 4.5): World's best coding model
- **1M context window**: Handle massive context easily
- **Agent-optimized**: Designed for multi-step workflows
- **90% prompt caching**: Essential for memory systems

### OpenAI Embeddings

| Model | Cost $/M | Dimensions | Quality | Use Case |
|-------|----------|------------|---------|----------|
| **text-embedding-3-small** | $0.02 | 1536 | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | **Recommended** (best value) |
| text-embedding-3-large | $0.13 | 3072 | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | Premium (6.5√ó more expensive) |

---

## Development Phase Strategy

### Goal: Fast, Cheap Iteration

During development, you'll be:
- Testing core functionality
- Debugging entity resolution
- Iterating on prompts
- Running hundreds of test queries
- Validating memory storage/retrieval

**You DON'T need premium quality yet** - save money and iterate fast.

### Recommended Configuration

```yaml
# Development Phase (Cheap & Fast)
primary_llm:
  model: gpt-4o-mini
  pricing: $0.15/$0.60 per M
  context: 128K
  quality: ‚≠ê‚≠ê‚≠ê‚≠ê

entity_extraction:
  model: gpt-5-nano
  pricing: $0.05/$0.40 per M
  context: 256K
  quality: ‚≠ê‚≠ê‚≠ê

embeddings:
  model: text-embedding-3-small
  pricing: $0.02 per M
  dimensions: 1536
  quality: ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê

caching: disabled  # Not needed during development
```

### Cost Breakdown (Development Phase)

**Typical Query** (25K input, 750 output tokens):
```
Entity Extraction (200 input, 75 output):
  ($0.05 √ó 0.2) + ($0.40 √ó 0.075) = $0.00001 + $0.00003 = $0.00004

Primary LLM (25K input, 750 output):
  ($0.15 √ó 25) + ($0.60 √ó 0.75) = $0.00375 + $0.00045 = $0.0042

Embeddings (200 tokens √ó 2):
  $0.02 √ó 0.4 = $0.000008

Total per query: ~$0.0042
```

**Development Phase Usage** (150 queries):
```
Testing & debugging:     100 queries √ó $0.0042 = $0.42
Feature development:      30 queries √ó $0.0042 = $0.13
Integration testing:      20 queries √ó $0.0042 = $0.08

Total: ~$0.63
```

### Why GPT-4o mini for Development?

**Pros:**
- ‚úÖ **Proven quality**: Battle-tested, reliable
- ‚úÖ **128K context**: Sufficient for all test scenarios
- ‚úÖ **Fast**: Low latency for quick iteration
- ‚úÖ **Cheap**: 20√ó cheaper than Claude Sonnet 4.5
- ‚úÖ **Simple API**: No caching configuration needed
- ‚úÖ **Good debugging**: Clear error messages

**vs GPT-5-mini:**
- GPT-4o mini is MORE proven (released earlier, more battle-tested)
- GPT-5-mini is newer (fewer known issues, but less tested)
- **Cost difference is minimal**: $0.15 vs $0.25 per M (~40% cheaper with 4o mini)

**Recommendation**: Start with **GPT-4o mini** for development.

---

## Final Iteration Strategy

### Goal: Maximum Quality for Demo

For your final 50 demo queries, you want:
- **Best possible response quality** to impress the interviewer
- **Sophisticated reasoning** for pattern analysis
- **Impressive context handling** (1M token window)
- **Professional polish** in all responses

**Now is when you switch to premium models.**

### Recommended Configuration

```yaml
# Final Iteration (Premium Quality)
primary_llm:
  model: claude-sonnet-4.5-20250929
  pricing: $3/$15 per M (with 90% cache discount)
  context: 1M tokens
  quality: ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê
  features:
    - prompt_caching: true
    - agent_optimized: true
    - hybrid_reasoning: true

entity_extraction:
  model: claude-3-5-haiku-20250219
  pricing: $0.80/$4 per M
  context: 200K
  quality: ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê

embeddings:
  model: text-embedding-3-small
  pricing: $0.02 per M
  dimensions: 1536
  quality: ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê

caching: enabled  # Critical for cost savings
```

### Cost Breakdown (Final Iteration)

**First Query** (no cache, 25K input, 750 output):
```
Entity Extraction: $0.00046
Primary LLM: ($3 √ó 25) + ($15 √ó 0.75) = $0.086
Embeddings: $0.000008

Total first query: $0.0865
```

**Subsequent Queries** (80% cached):
```
Entity Extraction: $0.00046

Primary LLM:
  Fresh content (20%): ($3 √ó 5) = $0.015
  Cached content (80%): ($0.30 √ó 20) = $0.006
  Output: ($15 √ó 0.75) = $0.01125
  Total: $0.0323

Embeddings: $0.000008

Total per query (cached): $0.0328
```

**Final Demo Phase** (50 queries):
```
First query (no cache):    1 √ó $0.0865 = $0.09
Next 49 queries (cached): 49 √ó $0.0328 = $1.61

Total: ~$1.70
```

### Why Claude Sonnet 4.5 for Final?

**Pros:**
- ‚úÖ **77.2% SWE-bench**: Best coding model in the world
- ‚úÖ **Agent-optimized**: Perfect for memory system workflows
- ‚úÖ **1M context**: Handles any scenario, shows sophistication
- ‚úÖ **Prompt caching**: 90% discount makes it cost-competitive
- ‚úÖ **Superior reasoning**: Multi-dimensional analysis, strategic insights
- ‚úÖ **Impressive responses**: Will wow the interviewer

**Cons:**
- ‚ö†Ô∏è First query is expensive ($0.086)
- ‚ö†Ô∏è Requires caching implementation

**vs GPT-5:**

| Feature | Claude Sonnet 4.5 | GPT-5 | Winner |
|---------|-------------------|-------|--------|
| SWE-bench | 77.2% | 74.9% | Claude |
| Context | 1M | 256K-1M | Claude |
| Cost (cached) | $0.033 | $0.039 | Claude |
| Agent optimization | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê | Claude |
| Reasoning | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê (94.6% AIME) | Tie |
| Latency | 2-3s | 1-2s | GPT-5 |

**Recommendation**: **Claude Sonnet 4.5** for final iteration due to superior coding, agent optimization, and cost with caching.

---

## Cost Comparison & Projections

### Development Phase (150 queries)

| Configuration | Primary LLM | Cost/Query | Total (150q) |
|---------------|-------------|-----------|--------------|
| **Recommended** | GPT-4o mini | $0.0042 | **$0.63** |
| Alternative 1 | GPT-5-mini | $0.0070 | $1.05 |
| Alternative 2 | GPT-5 | $0.0340 | $5.10 |
| Not recommended | Claude Sonnet 4.5 | $0.0865* | $12.98* |

*Without caching during development iteration

**Winner**: GPT-4o mini saves **$0.42** vs GPT-5-mini, **$4.47** vs GPT-5

### Final Iteration (50 queries)

| Configuration | Primary LLM | Cost/Query | Total (50q) |
|---------------|-------------|-----------|------------|
| **Recommended** | Claude Sonnet 4.5 (cached) | $0.033 | **$1.70** |
| Alternative 1 | GPT-5 (cached) | $0.039 | $1.95 |
| Alternative 2 | GPT-4o mini | $0.0042 | $0.21 |
| Alternative 3 | Claude Opus 4.1 | $0.431 | $21.55 |

**Winner**: Claude Sonnet 4.5 provides best quality at reasonable cost.

### Total Project Cost

```
Development Phase:
  150 queries √ó $0.0042 = $0.63

Final Iteration:
  50 queries √ó $0.033 = $1.70 (with caching)

Total Project: $2.33
```

### Cost Savings from Two-Phase Strategy

**Compare to always using premium:**
```
If you used Claude Sonnet 4.5 for all 200 queries:
  First query: $0.0865
  Next 199: $0.033 √ó 199 = $6.57
  Total: $6.66

Savings with two-phase: $6.66 - $2.33 = $4.33 (65% cheaper!)
```

**Compare to always using budget:**
```
If you used GPT-4o mini for all 200 queries:
  200 √ó $0.0042 = $0.84
  Total: $0.84

Extra investment for quality: $2.33 - $0.84 = $1.49
```

**ROI**: Spending an extra **$1.49** gets you world-class quality for your demo while keeping development cheap. **Absolutely worth it.**

---

## Quality Analysis

### Development Phase Quality (GPT-4o mini)

**Capabilities:**
- ‚úÖ Excellent for testing core functionality
- ‚úÖ Good entity extraction accuracy
- ‚úÖ Adequate context synthesis
- ‚úÖ Fast debugging feedback
- ‚úÖ 128K context handles all test scenarios

**Example Response** (Development):
```
User: "What's Delta Airlines' payment history?"

GPT-4o mini Response:
Delta Airlines Payment Summary:
‚Ä¢ Current outstanding: $45,230
‚Ä¢ Average payment cycle: 47 days (Net-45 terms)
‚Ä¢ Recent issues: 3 late payments in last 6 months
‚Ä¢ Payment reliability: 75%

Recommendation: Implement 30-day payment reminders and schedule
quarterly review to address Q4 delays.
```

**Quality**: ‚≠ê‚≠ê‚≠ê‚≠ê Good, functional, demonstrates capability

---

### Final Iteration Quality (Claude Sonnet 4.5)

**Capabilities:**
- ‚úÖ 77.2% SWE-bench (best coding model)
- ‚úÖ Sophisticated multi-dimensional analysis
- ‚úÖ Strategic recommendations with reasoning
- ‚úÖ 1M context for impressive demonstrations
- ‚úÖ Agent-optimized for complex workflows

**Example Response** (Final Demo):
```
User: "What's Delta Airlines' payment history?"

Claude Sonnet 4.5 Response:
Comprehensive Payment Analysis: Delta Airlines

üìä Performance Metrics:
‚Ä¢ Contract Terms: Net-45
‚Ä¢ Actual Payment Cycle: 47 days (avg) ‚Üí 4% above terms
‚Ä¢ Payment Reliability: 75% (below 85% benchmark)
‚Ä¢ Outstanding AR: $45,230 (18% of $250K credit limit)

üìà Temporal Patterns & Root Cause:
‚Ä¢ H1 2024: 43 days average (excellent performance)
‚Ä¢ H2 2024: 52 days average (deteriorating trend)
‚Ä¢ Q4 specifically: 58 days average

Root Cause Hypothesis:
The Q4 payment slowdown correlates with Delta's fiscal year-end
(December 31), suggesting internal budget freeze or approval bottlenecks.
This is a process issue, not a financial health concern.

üéØ Strategic Recommendations:

Immediate Actions (Week 1):
1. Contact Delta's AP team to understand Q4 workflow constraints
2. Implement automated payment reminders at 30-day mark
3. Add payment tracking to monthly business review dashboard

Process Optimization (Month 1-2):
4. Propose split payment terms for large Q4 orders:
   ‚Ä¢ 50% Net-30, 50% Net-60 (eases their Q4 cash flow)
5. Offer 2% early payment discount for payments within 30 days
6. Establish quarterly payment forecast meetings

Risk Mitigation (Ongoing):
7. Temporarily reduce credit limit to $200K until reliability improves
8. Flag account for manual review on orders exceeding $50K
9. Monitor for 3 consecutive on-time payments before terms expansion

Expected Outcomes:
‚Ä¢ Payment cycle improvement: 47 ‚Üí 42 days (-11%)
‚Ä¢ Reliability increase: 75% ‚Üí 90% (+15 percentage points)
‚Ä¢ AR reduction: $45K ‚Üí $30K (-33%)
‚Ä¢ Relationship status: Strengthened through proactive partnership

Assessment: This pattern is recoverable with proper process intervention.
The underlying financial health remains solid - this is about optimizing
their internal payment operations, not creditworthiness concerns.
```

**Quality**: ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê Strategic, comprehensive, impressive reasoning

**Quality Difference**: The Claude Sonnet 4.5 response is:
- **3√ó longer** with more detail
- **Multi-dimensional analysis** (metrics + patterns + root cause)
- **Phased recommendations** (immediate, optimization, mitigation)
- **Quantified outcomes** (specific improvement targets)
- **Strategic context** (distinguishes process vs financial issues)

**This is what impresses interviewers.**

---

## Implementation Guide

### Phase 1: Development Setup

```bash
# Install dependencies
pip install openai

# Set up environment
export OPENAI_API_KEY="sk-..."
export PHASE="development"  # or "final"
```

```python
# config/models.py

from typing import Literal
import os

Phase = Literal["development", "final"]

MODEL_CONFIGS = {
    "development": {
        "primary_llm": {
            "provider": "openai",
            "model": "gpt-4o-mini",
            "input_cost": 0.15,
            "output_cost": 0.60,
            "enable_caching": False,
        },
        "entity_extraction": {
            "provider": "openai",
            "model": "gpt-5-nano",
            "input_cost": 0.05,
            "output_cost": 0.40,
            "enable_caching": False,
        },
        "embeddings": {
            "provider": "openai",
            "model": "text-embedding-3-small",
            "cost": 0.02,
        },
    },
    "final": {
        "primary_llm": {
            "provider": "anthropic",
            "model": "claude-sonnet-4.5-20250929",
            "input_cost": 3.00,
            "output_cost": 15.00,
            "enable_caching": True,
            "cache_read_cost": 0.30,
        },
        "entity_extraction": {
            "provider": "anthropic",
            "model": "claude-3-5-haiku-20250219",
            "input_cost": 0.80,
            "output_cost": 4.00,
            "enable_caching": False,
        },
        "embeddings": {
            "provider": "openai",
            "model": "text-embedding-3-small",
            "cost": 0.02,
        },
    },
}


def get_config(phase: Phase = None):
    """Get model configuration for current phase"""
    if phase is None:
        phase = os.getenv("PHASE", "development")
    return MODEL_CONFIGS[phase]
```

### Phase 2: Unified Client

```python
# core/llm/client.py

import os
from openai import OpenAI
from anthropic import Anthropic

class LLMClient:
    """Unified client for OpenAI and Anthropic models"""

    def __init__(self, phase: Phase = "development"):
        self.phase = phase
        self.config = get_config(phase)

        # Initialize clients
        self.openai = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
        self.anthropic = Anthropic(api_key=os.getenv("ANTHROPIC_API_KEY"))

    def complete(
        self,
        messages: list[dict],
        model_type: Literal["primary_llm", "entity_extraction"] = "primary_llm",
        max_tokens: int = 2048,
        temperature: float = 0.7,
    ):
        """Generate completion"""

        config = self.config[model_type]
        provider = config["provider"]
        model = config["model"]

        if provider == "openai":
            return self._openai_complete(messages, model, max_tokens, temperature)
        elif provider == "anthropic":
            return self._anthropic_complete(
                messages, model, max_tokens, temperature,
                enable_caching=config.get("enable_caching", False)
            )
        else:
            raise ValueError(f"Unknown provider: {provider}")

    def _openai_complete(self, messages, model, max_tokens, temperature):
        """OpenAI completion"""

        response = self.openai.chat.completions.create(
            model=model,
            messages=messages,
            max_tokens=max_tokens,
            temperature=temperature
        )

        return {
            "content": response.choices[0].message.content,
            "usage": {
                "input_tokens": response.usage.prompt_tokens,
                "output_tokens": response.usage.completion_tokens,
                "cache_read_tokens": 0,
            },
            "model": model,
        }

    def _anthropic_complete(self, messages, model, max_tokens, temperature, enable_caching=False):
        """Anthropic completion with optional caching"""

        # Separate system prompt
        system_prompt = None
        claude_messages = []

        for msg in messages:
            if msg["role"] == "system":
                system_prompt = msg["content"]
            else:
                claude_messages.append({
                    "role": msg["role"],
                    "content": msg["content"]
                })

        # Build request with caching if enabled
        kwargs = {
            "model": model,
            "max_tokens": max_tokens,
            "temperature": temperature,
            "messages": claude_messages,
        }

        if enable_caching and system_prompt:
            # Mark system prompt for caching
            kwargs["system"] = [{
                "type": "text",
                "text": system_prompt,
                "cache_control": {"type": "ephemeral"}
            }]
        elif system_prompt:
            kwargs["system"] = system_prompt

        response = self.anthropic.messages.create(**kwargs)

        return {
            "content": response.content[0].text,
            "usage": {
                "input_tokens": response.usage.input_tokens,
                "output_tokens": response.usage.output_tokens,
                "cache_read_tokens": getattr(response.usage, "cache_read_input_tokens", 0),
                "cache_write_tokens": getattr(response.usage, "cache_creation_input_tokens", 0),
            },
            "model": model,
        }

    def embed(self, texts: list[str]) -> list[list[float]]:
        """Generate embeddings (always OpenAI)"""

        config = self.config["embeddings"]
        model = config["model"]

        response = self.openai.embeddings.create(
            model=model,
            input=texts
        )

        return [item.embedding for item in response.data]


# Usage
client = LLMClient(phase="development")  # or "final"

response = client.complete(
    messages=[
        {"role": "system", "content": "You are a helpful assistant."},
        {"role": "user", "content": "What's Acme Corp's phone number?"}
    ],
    model_type="primary_llm"
)

print(response["content"])
```

### Phase 3: Cost Tracking

```python
# core/monitoring/cost_tracker.py

class CostTracker:
    """Track costs across phases"""

    def __init__(self):
        self.costs = {
            "development": [],
            "final": []
        }
        self.current_phase = os.getenv("PHASE", "development")

    def track(self, usage: dict, config: dict, phase: str = None):
        """Track a request's cost"""

        if phase is None:
            phase = self.current_phase

        # Calculate costs
        input_cost = (usage["input_tokens"] / 1_000_000) * config["input_cost"]
        output_cost = (usage["output_tokens"] / 1_000_000) * config["output_cost"]

        # Apply cache discount if applicable
        cache_cost = 0
        if usage.get("cache_read_tokens", 0) > 0:
            cache_discount = 0.9  # 90% off
            cache_read_cost = config.get("cache_read_cost", config["input_cost"] * 0.1)
            cache_cost = (usage["cache_read_tokens"] / 1_000_000) * cache_read_cost
            input_cost = ((usage["input_tokens"] - usage["cache_read_tokens"]) / 1_000_000) * config["input_cost"]

        total_cost = input_cost + output_cost + cache_cost

        self.costs[phase].append({
            "timestamp": datetime.now(),
            "model": config["model"],
            "input_tokens": usage["input_tokens"],
            "output_tokens": usage["output_tokens"],
            "cache_read_tokens": usage.get("cache_read_tokens", 0),
            "cost": total_cost
        })

        return total_cost

    def get_summary(self):
        """Get cost summary by phase"""

        summary = {}
        for phase, requests in self.costs.items():
            if requests:
                summary[phase] = {
                    "total_cost": sum(r["cost"] for r in requests),
                    "total_requests": len(requests),
                    "avg_cost": sum(r["cost"] for r in requests) / len(requests),
                    "total_tokens": sum(r["input_tokens"] + r["output_tokens"] for r in requests),
                }

        # Overall summary
        total_cost = sum(s["total_cost"] for s in summary.values())
        total_requests = sum(s["total_requests"] for s in summary.values())

        summary["overall"] = {
            "total_cost": total_cost,
            "total_requests": total_requests,
            "avg_cost": total_cost / total_requests if total_requests else 0,
        }

        return summary


# Usage
tracker = CostTracker()

# During request
cost = tracker.track(response["usage"], client.config["primary_llm"])
print(f"Cost: ${cost:.6f}")

# At end of project
summary = tracker.get_summary()
print(f"Development phase: ${summary['development']['total_cost']:.2f}")
print(f"Final phase: ${summary['final']['total_cost']:.2f}")
print(f"Total project: ${summary['overall']['total_cost']:.2f}")
```

---

## Migration Path

### Step 1: Start with Development Phase

```bash
# Set environment
export PHASE="development"
export OPENAI_API_KEY="sk-..."

# Build and test system
python main.py
```

**During this phase:**
- Build core functionality
- Test all 18 scenarios
- Debug issues
- Iterate on prompts
- Validate memory storage/retrieval
- Run integration tests

**Estimated**: 150 queries, ~$0.63 cost

---

### Step 2: Switch to Final Iteration

```bash
# Install Anthropic SDK
pip install anthropic

# Update environment
export PHASE="final"
export ANTHROPIC_API_KEY="sk-ant-..."
export OPENAI_API_KEY="sk-..."  # Still needed for embeddings

# No code changes needed - client auto-switches based on PHASE
python main.py
```

**During this phase:**
- Run final end-to-end tests
- Prepare demo scenarios
- Generate demo queries for presentation
- Polish responses
- Record demo video (if needed)

**Estimated**: 50 queries, ~$1.70 cost

---

### Step 3: Demo Day

```bash
# Ensure you're in final phase
export PHASE="final"

# Run demo
python demo.py
```

**Tips for demo:**
- Show a mix of simple and complex queries
- Highlight the 1M context window capability
- Demonstrate pattern analysis (Scenario 14)
- Show strategic recommendations
- Mention you used Claude Sonnet 4.5 (77.2% SWE-bench)

---

## Benchmarks & Performance

### GPT-4o mini (Development)

| Benchmark | Score | Rank |
|-----------|-------|------|
| MMLU | ~82% | Good |
| HumanEval (Coding) | ~85% | Good |
| Context Window | 128K | Adequate |
| Speed | Fast | Excellent |
| Cost-Performance | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | Best in class |

### Claude Sonnet 4.5 (Final)

| Benchmark | Score | Rank |
|-----------|-------|------|
| SWE-bench | 77.2% | #1 World |
| MMLU | ~85% | Excellent |
| Context Window | 1M | Best available |
| Agent Tasks | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | Optimized |
| Reasoning Quality | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | Top tier |

### GPT-5 (Alternative Final)

| Benchmark | Score | Rank |
|-----------|-------|------|
| SWE-bench | 74.9% | #2 World |
| AIME (Math) | 94.6% | #1 Math |
| MMLU | ~88% | Excellent |
| Context Window | 256K-1M | Excellent |
| Reasoning Router | Auto | Convenient |

---

## FAQ

### Q: Why not use Claude for development too?

**A:** Claude 3.5 Haiku ($0.80/$4) is 5√ó more expensive than GPT-4o mini ($0.15/$0.60) for similar development-phase quality. Save that money for the final iteration where it matters.

### Q: Can I use GPT-5 instead of Claude for final?

**A:** Yes! GPT-5 is excellent:
- **Pros**: Automatic reasoning routing, slightly cheaper base cost, unified OpenAI ecosystem
- **Cons**: Claude Sonnet 4.5 has better SWE-bench (77.2% vs 74.9%) and agent optimization

**Both are great choices** - Claude edges out slightly for coding tasks.

### Q: What if I don't want to switch models mid-project?

**A:** Use **GPT-4o mini** for everything:
- Total cost: 200 √ó $0.0042 = **$0.84**
- Quality: ‚≠ê‚≠ê‚≠ê‚≠ê (good, but not wow)
- Pros: Simple, cheap, no switching
- Cons: Responses won't be as impressive

**Trade-off**: Save $1.49 but miss out on wow factor.

### Q: Is prompt caching complicated to set up?

**A:** No, it's simple:

```python
# Anthropic caching (automatic with client above)
response = client.complete(
    messages=[
        {"role": "system", "content": system_prompt},  # Auto-cached
        {"role": "user", "content": user_query}
    ],
    model_type="primary_llm"
)
```

The client handles caching automatically when `enable_caching=True`.

### Q: When should I switch from development to final?

**A:** Switch when:
- ‚úÖ All core functionality working
- ‚úÖ All 18 scenarios passing
- ‚úÖ No major bugs remaining
- ‚úÖ Ready to prepare demo
- ‚úÖ ~1-2 days before submission

**Typical timeline**:
- Days 1-10: Development phase
- Days 11-12: Final iteration + demo prep
- Day 13: Submission

---

## Summary & Recommendations

### For Your Take-Home Project

**Development Phase** (Days 1-10):
```yaml
Primary LLM: GPT-4o mini        # $0.15/$0.60 per M
Entity Extraction: GPT-5-nano   # $0.05/$0.40 per M
Embeddings: text-embedding-3-small # $0.02 per M

Budget: ~$0.63 for 150 queries
```

**Final Iteration** (Days 11-13):
```yaml
Primary LLM: Claude Sonnet 4.5  # $3/$15 per M (cached $0.30)
Entity Extraction: Claude 3.5 Haiku # $0.80/$4 per M
Embeddings: text-embedding-3-small # $0.02 per M

Budget: ~$1.70 for 50 queries
```

**Total Project Cost**: **$2.33**

### Key Benefits of Two-Phase Strategy

1. **Cost Efficient** (65% cheaper than always-premium)
2. **Fast Development** (cheap models = guilt-free iteration)
3. **Impressive Demo** (premium quality when it matters)
4. **Simple Migration** (change one environment variable)
5. **Best of Both Worlds** (speed + quality)

### Action Items

**Week 1-2: Development**
- [ ] Install OpenAI SDK
- [ ] Set `PHASE=development`
- [ ] Build core functionality with GPT-4o mini
- [ ] Test all 18 scenarios
- [ ] Debug and iterate freely
- [ ] Track costs (should be ~$0.50-0.70)

**Final Days: Polish**
- [ ] Install Anthropic SDK
- [ ] Set `PHASE=final`
- [ ] Run final tests with Claude Sonnet 4.5
- [ ] Prepare demo scenarios
- [ ] Generate impressive responses
- [ ] Record demo (if needed)
- [ ] Submit project

**Total Expected Cost**: $2-3

---

## Conclusion

The **two-phase strategy** is optimal for take-home projects:

- **Development**: Fast, cheap iteration with GPT-4o mini ($0.63)
- **Final**: Premium quality with Claude Sonnet 4.5 ($1.70)
- **Total**: $2.33 (affordable + impressive)

**You get:**
- ‚úÖ Guilt-free development iteration
- ‚úÖ World-class demo quality (77.2% SWE-bench)
- ‚úÖ 1M context window to show off
- ‚úÖ Cost savings vs always-premium (65%)
- ‚úÖ Flexibility to switch back if needed

**This is the smart way to build a take-home project.**

---

**Document Version**: 1.0 (Consolidated Development Strategy)
**Last Updated**: October 15, 2025
**Providers**: Anthropic (Claude) + OpenAI (GPT) only
**Phase Strategy**: Development (cheap) ‚Üí Final (premium)
